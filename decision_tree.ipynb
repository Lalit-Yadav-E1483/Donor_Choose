{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for_sentiment = 'a person is a person no matter how small dr seuss i teach the smallest students with the biggest enthusiasm \\\n",
    "for learning my students learn in many different ways using all of our senses and multiple intelligences i use a wide range\\\n",
    "of techniques to help all my students succeed students in my class come from a variety of different backgrounds which makes\\\n",
    "for wonderful sharing of experiences and cultures including native americans our school is a caring community of successful \\\n",
    "learners which can be seen through collaborative student project based learning in and out of the classroom kindergarteners \\\n",
    "in my class love to work with hands on materials and have many different opportunities to practice a skill before it is\\\n",
    "mastered having the social skills to work cooperatively with friends is a crucial aspect of the kindergarten curriculum\\\n",
    "montana is the perfect place to learn about agriculture and nutrition my students love to role play in our pretend kitchen\\\n",
    "in the early childhood classroom i have had several kids ask me can we try cooking with real food i will take their idea \\\n",
    "and create common core cooking lessons where we learn important math and writing concepts while cooking delicious healthy \\\n",
    "food for snack time my students will have a grounded appreciation for the work that went into making the food and knowledge \\\n",
    "of where the ingredients came from as well as how it is healthy for their bodies this project would expand our learning of \\\n",
    "nutrition and agricultural cooking recipes by having us peel our own apples to make homemade applesauce make our own bread \\\n",
    "and mix up healthy plants from our classroom garden in the spring we will also create our own cookbooks to be printed and \\\n",
    "shared with families students will gain math and literature skills as well as a life long enjoyment for healthy cooking \\\n",
    "nannan'\n",
    "ss = sid.polarity_scores(for_sentiment)\n",
    "\n",
    "for k in ss:\n",
    "    print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "\n",
    "# we can use these 4 things as features/attributes (neg, neu, pos, compound)\n",
    "# neg: 0.0, neu: 0.753, pos: 0.247, compound: 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentiment_analysis(text):\n",
    "    ss = sid.polarity_scores(text)\n",
    "    for k in ss:\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_analysis('fuck you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for_sentiment = 'a person is a person no matter how small dr seuss i teach the smallest students with the biggest enthusiasm \\\n",
    "for learning my students learn in many different ways using all of our senses and multiple intelligences i use a wide range\\\n",
    "of techniques to help all my students succeed students in my class come from a variety of different backgrounds which makes\\\n",
    "for wonderful sharing of experiences and cultures including native americans our school is a caring community of successful \\\n",
    "learners which can be seen through collaborative student project based learning in and out of the classroom kindergarteners \\\n",
    "in my class love to work with hands on materials and have many different opportunities to practice a skill before it is\\\n",
    "mastered having the social skills to work cooperatively with friends is a crucial aspect of the kindergarten curriculum\\\n",
    "montana is the perfect place to learn about agriculture and nutrition my students love to role play in our pretend kitchen\\\n",
    "in the early childhood classroom i have had several kids ask me can we try cooking with real food i will take their idea \\\n",
    "and create common core cooking lessons where we learn important math and writing concepts while cooking delicious healthy \\\n",
    "food for snack time my students will have a grounded appreciation for the work that went into making the food and knowledge \\\n",
    "of where the ingredients came from as well as how it is healthy for their bodies this project would expand our learning of \\\n",
    "nutrition and agricultural cooking recipes by having us peel our own apples to make homemade applesauce make our own bread \\\n",
    "and mix up healthy plants from our classroom garden in the spring we will also create our own cookbooks to be printed and \\\n",
    "shared with families students will gain math and literature skills as well as a life long enjoyment for healthy cooking \\\n",
    "nannan'\n",
    "ss = sid.polarity_scores(for_sentiment)\n",
    "\n",
    "for k in ss:\n",
    "    print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "\n",
    "# we can use these 4 things as features/attributes (neg, neu, pos, compound)\n",
    "# neg: 0.0, neu: 0.753, pos: 0.247, compound: 0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.sparse import hstack\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/preprocessed_df.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = df['project_is_approved'].values\n",
    "X = df.drop(['project_is_approved'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into Train and cross validation(or test): Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)\n",
    "print(X_train.shape, y_train.shape), print(X_cv.shape, y_cv.shape), print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Model Ready: encoding eassay, and project_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_all_features = []\n",
    "tfidf_w2v_all_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(min_df=10, ngram_range=(1,4), max_features=20000)\n",
    "\n",
    "X_train_essay_tfidf = vectorizer_tfidf.fit_transform(X_train['essay'].values)\n",
    "X_cv_essay_tfidf = vectorizer_tfidf.transform(X_cv['essay'].values)\n",
    "X_test_essay_tfidf = vectorizer_tfidf.transform(X_test['essay'].values)\n",
    "\n",
    "print(X_train_essay_tfidf.shape, y_train.shape)\n",
    "print(X_cv_essay_tfidf.shape, y_cv.shape)\n",
    "print(X_test_essay_tfidf.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "\n",
    "tfidf_all_features.extend(vectorizer_tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_sentence_train=[]\n",
    "for sentence in X_train['essay']:\n",
    "    list_of_sentence_train.append(sentence.split())\n",
    "\n",
    "w2v_model = Word2Vec(list_of_sentence_train, min_count=10, size=50, workers=4)\n",
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "tf_idf_vect = TfidfVectorizer(min_df=10, ngram_range=(1,4), max_features=20000)\n",
    "tf_idf_matrix = tf_idf_vect.fit_transform(X_train['essay'])\n",
    "\n",
    "tfidf_feat = tf_idf_vect.get_feature_names() \n",
    "dictionary = dict(zip(tf_idf_vect.get_feature_names(), list(tf_idf_vect.idf_)))\n",
    "\n",
    "tfidf_w2v_train = []\n",
    "row = 0\n",
    "for sentence in tqdm(list_of_sentence_train): \n",
    "    sentence_vec = np.zeros(50)\n",
    "    weight_sum = 0\n",
    "    for word in sentence:\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model.wv[word]\n",
    "            tf_idf = dictionary[word] * (sentence.count(word) / len(sentence))\n",
    "            sentence_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sentence_vec /= weight_sum\n",
    "    tfidf_w2v_train.append(sentence_vec)\n",
    "    row += 1\n",
    "\n",
    "    \n",
    "list_of_sentence_cv = []\n",
    "for sentence in X_cv['essay']:\n",
    "    list_of_sentence_cv.append(sentence.split())\n",
    "\n",
    "tfidf_w2v_cv = []\n",
    "row = 0\n",
    "for sentence in tqdm(list_of_sentence_cv): \n",
    "    sentence_vec = np.zeros(50)\n",
    "    weight_sum = 0\n",
    "    for word in sentence:\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model.wv[word]\n",
    "            tf_idf = dictionary[word] * (sentence.count(word) / len(sentence))\n",
    "            sentence_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sentence_vec /= weight_sum\n",
    "    tfidf_w2v_cv.append(sentence_vec)\n",
    "    row += 1\n",
    "    \n",
    "list_of_sentence_test = []\n",
    "for sentence in X_test['essay']:\n",
    "    list_of_sentence_test.append(sentence.split())\n",
    "\n",
    "tfidf_w2v_test = []\n",
    "row = 0\n",
    "for sentence in tqdm(list_of_sentence_test): \n",
    "    sentence_vec = np.zeros(50)\n",
    "    weight_sum = 0\n",
    "    for word in sentence:\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model.wv[word]\n",
    "            tf_idf = dictionary[word] * (sentence.count(word) / len(sentence))\n",
    "            sentence_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sentence_vec /= weight_sum\n",
    "    tfidf_w2v_test.append(sentence_vec)\n",
    "    row += 1\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_w2v = np.array([tfidf_w2v_train, tfidf_w2v_cv, tfidf_w2v_test])\n",
    "tfidf = np.array([X_train_essay_tfidf, X_cv_essay_tfidf, X_test_essay_tfidf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('tmp/vectorizer_tfidf', 'wb') as f:\n",
    "    pickle.dump(vectorizer_tfidf.get_feature_names(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_w2v_all_features.extend(w2v_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer_tfidf_item = TfidfVectorizer(min_df=10, ngram_range=(1,4), max_features=5000)\n",
    "\n",
    "X_train_title_tfidf = vectorizer_tfidf_item.fit_transform(X_train['project_title'].values.astype('U'))\n",
    "X_cv_title_tfidf = vectorizer_tfidf_item.transform(X_cv['project_title'].values.astype('U'))\n",
    "X_test_title_tfidf = vectorizer_tfidf_item.transform(X_test['project_title'].values.astype('U'))\n",
    "\n",
    "print(X_train_title_tfidf.shape, y_train.shape)\n",
    "print(X_cv_title_tfidf.shape, y_cv.shape)\n",
    "print(X_test_title_tfidf.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "\n",
    "tfidf_all_features.extend(vectorizer_tfidf_item.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train['project_title'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_title_train=[]\n",
    "for sentence in X_train['project_title'].values.astype('U'):\n",
    "    list_of_title_train.append(sentence.split())\n",
    "\n",
    "w2v_model = Word2Vec(list_of_title_train, min_count=10, size=50, workers=4)\n",
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "tf_idf_vect = TfidfVectorizer(min_df=10, ngram_range=(1,4), max_features=20000)\n",
    "tf_idf_matrix = tf_idf_vect.fit_transform(X_train['project_title'].values.astype('U'))\n",
    "\n",
    "tfidf_feat = tf_idf_vect.get_feature_names() \n",
    "dictionary = dict(zip(tf_idf_vect.get_feature_names(), list(tf_idf_vect.idf_)))\n",
    "\n",
    "tfidf_w2v_title_train = []\n",
    "row = 0\n",
    "for sentence in tqdm(list_of_title_train): \n",
    "    sentence_vec = np.zeros(50)\n",
    "    weight_sum = 0\n",
    "    for word in sentence:\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model.wv[word]\n",
    "            tf_idf = dictionary[word] * (sentence.count(word) / len(sentence))\n",
    "            sentence_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sentence_vec /= weight_sum\n",
    "    tfidf_w2v_title_train.append(sentence_vec)\n",
    "    row += 1\n",
    "\n",
    "    \n",
    "list_of_title_cv = []\n",
    "for sentence in X_cv['project_title'].values.astype('U'):\n",
    "    list_of_title_cv.append(sentence.split())\n",
    "\n",
    "tfidf_w2v_title_cv = []\n",
    "row = 0\n",
    "for sentence in tqdm(list_of_title_cv): \n",
    "    sentence_vec = np.zeros(50)\n",
    "    weight_sum = 0\n",
    "    for word in sentence:\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model.wv[word]\n",
    "            tf_idf = dictionary[word] * (sentence.count(word) / len(sentence))\n",
    "            sentence_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sentence_vec /= weight_sum\n",
    "    tfidf_w2v_title_cv.append(sentence_vec)\n",
    "    row += 1\n",
    "    \n",
    "list_of_title_test = []\n",
    "for sentence in X_test['project_title'].values.astype('U'):\n",
    "    list_of_title_test.append(sentence.split())\n",
    "\n",
    "tfidf_w2v_title_test = []\n",
    "row = 0\n",
    "for sentence in tqdm(list_of_title_test): \n",
    "    sentence_vec = np.zeros(50)\n",
    "    weight_sum = 0\n",
    "    for word in sentence:\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model.wv[word]\n",
    "            tf_idf = dictionary[word] * (sentence.count(word) / len(sentence))\n",
    "            sentence_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sentence_vec /= weight_sum\n",
    "    tfidf_w2v_title_test.append(sentence_vec)\n",
    "    row += 1\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_w2v_all_features.extend(w2v_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_tfidf_w2v = np.array([tfidf_w2v_title_train, tfidf_w2v_title_cv, tfidf_w2v_title_test])\n",
    "title_tfidf = np.array([X_train_title_tfidf, X_cv_title_tfidf, X_test_title_tfidf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open('tmp/vectorizer_tfidf_item', 'wb') as f:\n",
    "#     pickle.dump(vectorizer_tfidf_item.get_feature_names(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variable school state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['school_state'].values)\n",
    "\n",
    "X_train_state_ohe = vectorizer.transform(X_train['school_state'].values)\n",
    "X_cv_state_ohe = vectorizer.transform(X_cv['school_state'].values)\n",
    "X_test_state_ohe = vectorizer.transform(X_test['school_state'].values)\n",
    "\n",
    "print(X_train_state_ohe.shape, y_train.shape)\n",
    "print(X_cv_state_ohe.shape, y_cv.shape)\n",
    "print(X_test_state_ohe.shape, y_test.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)\n",
    "\n",
    "tfidf_all_features.extend(vectorizer.get_feature_names())\n",
    "tfidf_w2v_all_features.extend(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding categorical features: teacher_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['teacher_prefix'].values)\n",
    "\n",
    "X_train_teacher_ohe = vectorizer.transform(X_train['teacher_prefix'].values)\n",
    "X_cv_teacher_ohe = vectorizer.transform(X_cv['teacher_prefix'].values)\n",
    "X_test_teacher_ohe = vectorizer.transform(X_test['teacher_prefix'].values)\n",
    "\n",
    "print(X_train_teacher_ohe.shape, y_train.shape)\n",
    "print(X_cv_teacher_ohe.shape, y_cv.shape)\n",
    "print(X_test_teacher_ohe.shape, y_test.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)\n",
    "\n",
    "tfidf_all_features.extend(vectorizer.get_feature_names())\n",
    "tfidf_w2v_all_features.extend(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['project_grade_category'].values)\n",
    "\n",
    "X_train_project_ohe = vectorizer.transform(X_train['project_grade_category'].values)\n",
    "X_cv_project_ohe = vectorizer.transform(X_cv['project_grade_category'].values)\n",
    "X_test_project_ohe = vectorizer.transform(X_test['project_grade_category'].values)\n",
    "\n",
    "print(X_train_project_ohe.shape, y_train.shape)\n",
    "print(X_cv_project_ohe.shape, y_cv.shape)\n",
    "print(X_test_project_ohe.shape, y_test.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)\n",
    "\n",
    "tfidf_all_features.extend(vectorizer.get_feature_names())\n",
    "tfidf_w2v_all_features.extend(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "\n",
    "normalizer.fit(X_train['price'].values.reshape(-1, 1))\n",
    "\n",
    "X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(-1, 1))\n",
    "X_cv_price_norm = normalizer.transform(X_cv['price'].values.reshape(-1, 1))\n",
    "X_test_price_norm = normalizer.transform(X_test['price'].values.reshape(-1, 1))\n",
    "\n",
    "print(X_train_price_norm.shape, y_train.shape)\n",
    "print(X_cv_price_norm.shape, y_cv.shape)\n",
    "print(X_test_price_norm.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "\n",
    "tfidf_all_features.extend(vectorizer.get_feature_names())\n",
    "tfidf_w2v_all_features.extend(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "\n",
    "normalizer.fit(X_train['teacher_number_of_previously_posted_projects'].values.reshape(1, -1))\n",
    "\n",
    "X_train_prev_proj_norm = normalizer.transform(X_train['teacher_number_of_previously_posted_projects'].values.reshape(-1, 1))\n",
    "X_cv_prev_proj_norm = normalizer.transform(X_cv['teacher_number_of_previously_posted_projects'].values.reshape(-1, 1))\n",
    "X_test_prev_proj_norm = normalizer.transform(X_test['teacher_number_of_previously_posted_projects'].values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "print(X_train_prev_proj_norm.shape, y_train.shape)\n",
    "print(X_cv_prev_proj_norm.shape, y_cv.shape)\n",
    "print(X_test_prev_proj_norm.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "\n",
    "tfidf_all_features.extend(vectorizer.get_feature_names())\n",
    "tfidf_w2v_all_features.extend(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = hstack((X_train_essay_tfidf, X_train_title_tfidf, X_train_state_ohe, X_train_teacher_ohe, X_train_project_ohe, X_train_price_norm, X_train_prev_proj_norm)).tocsr()\n",
    "X_cv_tfidf = hstack((X_cv_essay_tfidf, X_cv_title_tfidf, X_cv_state_ohe, X_cv_teacher_ohe, X_cv_project_ohe, X_cv_price_norm, X_cv_prev_proj_norm)).tocsr()\n",
    "X_test_tfidf = hstack((X_test_essay_tfidf, X_test_title_tfidf, X_test_state_ohe, X_test_teacher_ohe, X_test_project_ohe, X_test_price_norm, X_test_prev_proj_norm)).tocsr()\n",
    "\n",
    "print(\"Final Data matrix\")\n",
    "print(X_train_tfidf.shape, y_train.shape)\n",
    "print(X_cv_tfidf.shape, y_cv.shape)\n",
    "print(X_test_tfidf.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for TFIDF-W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_tfidf_w2v = hstack((tfidf_w2v_train, tfidf_w2v_title_train, X_train_state_ohe, X_train_teacher_ohe, X_train_project_ohe, X_train_price_norm, X_train_prev_proj_norm)).tocsr()\n",
    "X_cv_tfidf_w2v = hstack((tfidf_w2v_cv, tfidf_w2v_title_cv, X_cv_state_ohe, X_cv_teacher_ohe, X_cv_project_ohe, X_cv_price_norm, X_cv_prev_proj_norm)).tocsr()\n",
    "X_test_tfidf_w2v = hstack((tfidf_w2v_test, tfidf_w2v_title_test, X_test_state_ohe, X_test_teacher_ohe, X_test_project_ohe, X_test_price_norm, X_test_prev_proj_norm)).tocsr()\n",
    "\n",
    "print(\"Final Data matrix\")\n",
    "print(X_train_tfidf_w2v.shape, y_train.shape)\n",
    "print(X_cv_tfidf_w2v.shape, y_cv.shape)\n",
    "print(X_test_tfidf_w2v.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Decision tree on processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_model(X_train, Y_train):\n",
    "    parameters = { 'max_depth': [1, 5] }\n",
    "    model = DecisionTreeClassifier()\n",
    "    clf = GridSearchCV(model, parameters, cv=3, scoring=make_scorer(roc_auc_score), return_train_score=True, verbose=1, n_jobs=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_auc_mean = clf.cv_results_['mean_train_score']\n",
    "    train_auc_std = clf.cv_results_['std_train_score']\n",
    "    cv_auc_mean = clf.cv_results_['mean_test_score'] \n",
    "    cv_auc_std = clf.cv_results_['std_test_score']\n",
    "    optimal_alpha = clf.best_params_['max_depth']\n",
    "    return parameters['max_depth'], optimal_alpha, train_auc_mean, train_auc_std, cv_auc_mean, cv_auc_std\n",
    "\n",
    "def plot_cross_validate(alphas, train_auc_mean, train_auc_std, cv_auc_mean, cv_auc_std):\n",
    "    # this code is refered from: https://stackoverflow.com/a/48803361/4084039\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.plot(alphas, train_auc_mean, label='Train AUC')\n",
    "    plt.semilogx(alphas, train_auc_mean, label='Mean Train score', color='blue')\n",
    "    plt.gca().fill_between(alphas, train_auc_mean - train_auc_std, train_auc_mean + train_auc_std, alpha=0.3, color='blue')\n",
    "\n",
    "    \n",
    "    plt.plot(alphas, cv_auc_mean, label='CV AUC')\n",
    "    plt.semilogx(alphas, cv_auc_mean, label='Mean CV score', color='red')\n",
    "    plt.gca().fill_between(alphas, cv_auc_mean - cv_auc_std, cv_auc_mean + cv_auc_std, alpha=0.3, color='red')\n",
    "\n",
    "    plt.scatter(alphas, train_auc_mean, label='Train AUC points')\n",
    "    plt.scatter(alphas, cv_auc_mean, label='CV AUC points')\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"alpha: hyperparameter\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.title(\"alpha: hyperparameter v/s AUC\")\n",
    "    plt.grid(color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def model_auc_roc_score(X_train, y_train, X_test, y_test, optimal_alpha):\n",
    "    model = DecisionTreeClassifier(max_depth=optimal_alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_proba = model.predict_proba(X_train)[:,1]\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_proba)\n",
    "    test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_proba)\n",
    "    return model, y_train_proba, train_fpr, train_tpr, train_thresholds, y_test_proba, test_fpr, test_tpr, test_thresholds\n",
    "\n",
    "def plot_auc_roc_score(train_fpr, train_tpr, test_fpr, test_tpr):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    plt.plot(train_fpr, train_tpr, label=f\"Train AUC = {auc(train_fpr, train_tpr)}\")\n",
    "    plt.plot(test_fpr, test_tpr, label=f\"Test AUC = {auc(test_fpr, test_tpr)}\")\n",
    "    plt.plot([0, 1], [0, 1], 'g--')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"False Positive Rate(FPR)\")\n",
    "    plt.ylabel(\"True Positive Rate(TPR)\")\n",
    "    plt.title(\"AUC\")\n",
    "    plt.grid(color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "def predict(proba, thresholds, tpr, fpr):\n",
    "    pred = []\n",
    "    best_thres = thresholds[np.argmax(tpr * (1 - fpr))]\n",
    "    for prob in proba:\n",
    "        if prob >= best_thres:\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "    return best_thres, pred\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    df = pd.DataFrame(cm)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(df, annot=True, fmt='d')\n",
    "    plt.title(\"Confusion Matrix for test data\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "    \n",
    "def get_top_and_worst_features(model, all_features, num_of_features=20):\n",
    "    pos_indx = model.feature_importances_.argsort()[::-1][:num_of_features]\n",
    "    neg_indx = model.feature_importances_.argsort()[:num_of_features]\n",
    "    top_features = []\n",
    "    worst_features = []\n",
    "    for i in list(pos_indx[:num_of_features]):\n",
    "        top_features.append(all_features[i])\n",
    "\n",
    "    for i in list(neg_indx[:num_of_features]):\n",
    "        worst_features.append(all_features[i])\n",
    "        \n",
    "    return top_features, worst_features\n",
    "\n",
    "\n",
    "def build_word_cloud(features):\n",
    "    features = ' '.join(features)\n",
    "    wordcloud = WordCloud().generate(features)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def build_model(X_train, y_train, X_test, y_test, all_features, num_of_features=20):\n",
    "    alphas, optimal_alpha, train_auc_mean, train_auc_std, cv_auc_mean, cv_auc_std = cross_validate_model(X_train, y_train)\n",
    "    plot_cross_validate(alphas, train_auc_mean, train_auc_std, cv_auc_mean, cv_auc_std)\n",
    "    model, y_train_proba, train_fpr, train_tpr, train_thresholds, y_test_proba, test_fpr, test_tpr, test_thresholds = model_auc_roc_score(X_train, y_train, X_test, y_test, optimal_alpha)\n",
    "#     pdb.set_trace();\n",
    "    plot_auc_roc_score(train_fpr, train_tpr, test_fpr, test_tpr)\n",
    "    thresh, y_pred = predict(y_test_proba, test_thresholds, test_tpr, test_fpr)\n",
    "    print(f'The best threshold value:: {thresh}')\n",
    "    plot_confusion_matrix(y_test, y_pred)\n",
    "    top_features, worst_features = get_top_and_worst_features(model, all_features, num_of_features)\n",
    "    build_word_cloud(top_features)\n",
    "    build_word_cloud(worst_features)\n",
    "    return optimal_alpha, train_fpr, train_tpr, test_fpr, test_tpr\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimal_alpha_tfidf, train_fpr_tfidf, train_tpr_tfidf, test_fpr_tfidf, test_tpr_tfidf = build_model(X_train_tfidf, y_train, X_test_tfidf, y_test, tfidf_all_features, num_of_features=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "    \n",
    "x = PrettyTable()\n",
    "\n",
    "x.field_names = [\"Vectorizer\", \"Model\", \"Hyperparameter: Alpha\", \"Train AUC\", \"Test AUC\"]\n",
    "# auc_train_bow = auc(train_fpr_bow, train_tpr_bow)\n",
    "# auc_test_bow = auc(test_fpr_bow, test_tpr_bow)\n",
    "\n",
    "auc_train_tfidf = auc(train_fpr_tfidf, train_tpr_tfidf)\n",
    "auc_test_tfidf = auc(test_fpr_tfidf, test_tpr_tfidf)\n",
    "\n",
    "\n",
    "# x.add_row([\"BOW\", \"Multinomial Naive Bayes\", optimal_alpha_bow, round(auc_train_bow, 2),round(auc_test_bow, 2)])\n",
    "x.add_row([\"TF-IDF\", \"Decision Tree\", optimal_alpha_tfidf, round(auc_train_tfidf, 2),round(auc_test_tfidf, 2)])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
